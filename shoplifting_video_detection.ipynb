{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedchahed/Omdena/blob/main/shoplifting_video_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "31UO6Gresfbw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
            "  warnings.warn(\"No audio backend is available.\")\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint,LearningRateMonitor\n",
        "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau,CosineAnnealingWarmRestarts,OneCycleLR,CosineAnnealingLR\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Dataset,ConcatDataset,default_collate\n",
        "from sklearn.model_selection import KFold,GroupShuffleSplit,GroupKFold,LeaveOneGroupOut\n",
        "from torchmetrics import MeanAbsoluteError\n",
        "from sklearn.utils import shuffle\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchaudio import transforms as TA\n",
        "from sklearn.metrics import classification_report\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oK1QJsuOpPaa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"pytorchvideo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3ygqLLvnpZmm",
        "outputId": "e13774f4-78d7-4e23-da15-44861503bee6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Normal/Normal (1).mp4'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "normal_pth = \"Normal\"\n",
        "anom_pth = \"Shoplifting\"\n",
        "\n",
        "normal_files = [normal_pth + \"/\" + i for i in os.listdir(normal_pth)]\n",
        "anom_files = [anom_pth + \"/\" + i for i in os.listdir(anom_pth)]\n",
        "\n",
        "file_paths = normal_files + anom_files\n",
        "\n",
        "df = pd.DataFrame({'path': file_paths})\n",
        "df[\"path\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wyRYo5ARpfjA",
        "outputId": "ef49f3bc-bbfa-40af-e168-831472a3e4f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Normal/Normal (1).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Normal/Normal (10).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Normal/Normal (11).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Normal/Normal (12).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Normal/Normal (13).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>Shoplifting/Shoplifting (9).mp4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>Shoplifting/Shoplifting (90).mp4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>Shoplifting/Shoplifting (91).mp4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Shoplifting/Shoplifting (92).mp4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>Shoplifting/Shoplifting (93).mp4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>182 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 path  label\n",
              "0               Normal/Normal (1).mp4      0\n",
              "1              Normal/Normal (10).mp4      0\n",
              "2              Normal/Normal (11).mp4      0\n",
              "3              Normal/Normal (12).mp4      0\n",
              "4              Normal/Normal (13).mp4      0\n",
              "..                                ...    ...\n",
              "177   Shoplifting/Shoplifting (9).mp4      1\n",
              "178  Shoplifting/Shoplifting (90).mp4      1\n",
              "179  Shoplifting/Shoplifting (91).mp4      1\n",
              "180  Shoplifting/Shoplifting (92).mp4      1\n",
              "181  Shoplifting/Shoplifting (93).mp4      1\n",
              "\n",
              "[182 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"label\"] = [0]*len(normal_files) + [1]*len(anom_files)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g7qqnp5gqaCJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df,val_df=train_test_split(df,test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCVLr4o1pkDS",
        "outputId": "37699c12-2236-41b6-a425-6c9512d6af70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler, labeled_video_dataset\n",
        "\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    Normalize,\n",
        "    RandomShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    Permute\n",
        ")\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    # Lambda,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    Resize\n",
        ")\n",
        "\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LrEJIgTLp1z1"
      },
      "outputs": [],
      "source": [
        "#tuneable params\n",
        "num_video_samples=20\n",
        "video_duration=2\n",
        "model_name='efficient_x3d_xs'\n",
        "batch_size=8\n",
        "scheduler='cosine'\n",
        "clipmode='random'\n",
        "img_size=224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-cQsfO-oq3RS"
      },
      "outputs": [],
      "source": [
        "video_transform = Compose(\n",
        "            [\n",
        "            ApplyTransformToKey(\n",
        "              key=\"video\",\n",
        "              transform=Compose(\n",
        "                  [\n",
        "                    UniformTemporalSubsample(num_video_samples),\n",
        "                    # Lambda(lambda x: x / 255.0),\n",
        "                    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
        "                      #Determines the shorter spatial dim of the video (i.e. width or height) and scales it to the given size\n",
        "                    RandomShortSideScale(min_size=img_size+16, max_size=img_size+32),\n",
        "                    CenterCropVideo(img_size),\n",
        "                    RandomHorizontalFlip(p=0.5),\n",
        "                  ]\n",
        "                ),\n",
        "              ),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QhQZiK4FqM6n"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset,ConcatDataset,default_collate\n",
        "from torch.utils.data import DistributedSampler\n",
        "train_dataset=labeled_video_dataset(val_df,\n",
        "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
        "                    transform=video_transform, decode_audio=False\n",
        "                                   )\n",
        "\n",
        "train_loader=DataLoader(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "12LzcvIqrY_-"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "import torchvision.models as models\n",
        "import timm\n",
        "from pytorch_lightning import seed_everything, LightningModule, Trainer\n",
        "class OurModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super(OurModel,self).__init__()\n",
        "        self.validation_step_outputs = []\n",
        "        self.train_step_outputs = []\n",
        "        self.scheduler=scheduler\n",
        "        self.video_model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)\n",
        "        self.video_model.projection.model=nn.Linear(in_features=2048, out_features=1000, bias=True)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.linear=nn.Linear(1000,1)\n",
        "        self.lr=1e-3\n",
        "        self.batch_size=batch_size\n",
        "        self.numworker=0\n",
        "        self.metric = torchmetrics.Accuracy(task=\"binary\")\n",
        "        self.criterion=nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self,video):\n",
        "        x=self.video_model(video)\n",
        "        x=self.relu(x)\n",
        "        x=self.linear(x)\n",
        "        return x\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt=torch.optim.AdamW(params=self.parameters(),lr=self.lr )\n",
        "        if self.scheduler=='cosine':\n",
        "            scheduler=CosineAnnealingLR(opt,T_max=10,  eta_min=1e-6, last_epoch=-1)\n",
        "            return {'optimizer': opt,'lr_scheduler':scheduler}\n",
        "        elif self.scheduler=='reduce':\n",
        "            scheduler=ReduceLROnPlateau(opt,mode='min', factor=0.5, patience=5)\n",
        "            return {'optimizer': opt,'lr_scheduler':scheduler,'monitor':'val_loss'}\n",
        "        elif self.scheduler=='warm':\n",
        "            scheduler=CosineAnnealingWarmRestarts(opt,T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n",
        "            return {'optimizer': opt,'lr_scheduler':scheduler}\n",
        "        elif self.scheduler=='cycle':\n",
        "            opt=torch.optim.AdamW(params=self.parameters(),lr=1e-6 )\n",
        "            scheduler=OneCycleLR(opt,max_lr=1e-2,epochs=15,steps_per_epoch=len(self.train_df)//self.batch_size//4)\n",
        "            lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
        "            return {'optimizer': opt, 'lr_scheduler': lr_scheduler}\n",
        "        # elif self.scheduler=='lambda':\n",
        "        #     lambda1 = lambda epoch: 0.9 ** epoch\n",
        "        #     scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda1)\n",
        "        #     return {'optimizer': opt, 'lr_scheduler': scheduler}\n",
        "        elif self.scheduler=='constant':\n",
        "            return opt\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset=labeled_video_dataset(train_df,\n",
        "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
        "                    transform=video_transform, decode_audio=False)\n",
        "\n",
        "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
        "                   num_workers=self.numworker,\n",
        "                   pin_memory=True)\n",
        "        return loader\n",
        "\n",
        "    def training_step(self,batch,batch_idx):\n",
        "        video,label=batch['video'],batch['label']\n",
        "#         label=label.ravel().to(torch.int64)\n",
        "        out = self(video)\n",
        "        loss=self.criterion(out,label)\n",
        "        self.train_step_outputs.append(loss)\n",
        "        metric=self.metric(out,label.to(torch.int64))\n",
        "        return {'loss':loss,'metric':metric.detach()}\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # loss=torch.stack([x[\"loss\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
        "        # metric=torch.stack([x[\"metric\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
        "        # self.log('train_loss', loss,batch_size=self.batch_size)\n",
        "        # self.log('train_metric', metric,batch_size=self.batch_size)\n",
        "        # print('training loss ',self.current_epoch,loss,metric)\n",
        "        epoch_average = torch.stack(self.train_step_outputs).mean()\n",
        "        self.log(\"train_epoch_average\", epoch_average)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset=labeled_video_dataset(val_df,\n",
        "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
        "                    transform=video_transform, decode_audio=False)\n",
        "\n",
        "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
        "                   num_workers=self.numworker,\n",
        "                   pin_memory=True)\n",
        "        return loader\n",
        "\n",
        "    def validation_step(self,batch,batch_idx):\n",
        "        video,label=batch['video'],batch['label']\n",
        "        out = self(video)\n",
        "        loss=self.criterion(out,label)\n",
        "        metric=self.metric(out,label.to(torch.int64))\n",
        "        self.validation_step_outputs.append(loss)\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        return {'loss':loss,'metric':metric.detach()}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # loss=torch.stack([x[\"loss\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
        "        # metric=torch.stack([x[\"metric\"] for x in outputs]).mean().cpu().numpy().round(2)\n",
        "        # print('validation loss ',self.current_epoch,loss,metric)\n",
        "        # self.log('val_loss', loss,batch_size=self.batch_size)\n",
        "        # self.log('val_metric',metric,batch_size=self.batch_size)\n",
        "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
        "        self.log(\"validation_epoch_average\", epoch_average)\n",
        "        # self.validation_step_outputs.clear()  # free memory\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        dataset=labeled_video_dataset(val_df,\n",
        "                   clip_sampler=make_clip_sampler(clipmode, video_duration),\\\n",
        "                    transform=video_transform, decode_audio=False)\n",
        "\n",
        "        loader=DataLoader(dataset,batch_size=self.batch_size,\n",
        "                   num_workers=self.numworker,\n",
        "                   pin_memory=True)\n",
        "        return loader\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        video,label=batch['video'],batch['label']\n",
        "        out = self(video)\n",
        "        return { 'label': label.detach(), 'pred': out.detach()}\n",
        "\n",
        "    def on_test_epoch_end(self, outputs):\n",
        "        label = torch.cat([x['label'] for x in outputs]).cpu().numpy()\n",
        "        pred = torch.cat([x['pred'] for x in outputs]).cpu().numpy()\n",
        "        pred=np.where(pred>0.5,1,0)\n",
        "        print(classification_report(label, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ZxXhV2pmss0F"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(monitor='val_loss',dirpath='checkpoints',\n",
        "                                        filename='file',save_last=True)\n",
        "lr_monitor = LearningRateMonitor(logging_interval='epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTwndzy8s_lc",
        "outputId": "0397ea96-2650-4356-defc-657a4ad86882"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n",
            "Global seed set to 0\n",
            "Using 16bit None Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "model=OurModel()\n",
        "seed_everything(0)\n",
        "trainer = Trainer(max_epochs=30,\n",
        "#                 deterministic=True,\n",
        "                accelerator='gpu', devices=-1,\n",
        "                  precision=16,\n",
        "                accumulate_grad_batches=2,\n",
        "                enable_progress_bar = True,\n",
        "                num_sanity_val_steps=0,\n",
        "                  callbacks=[lr_monitor,checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8oC36bau97-",
        "outputId": "9fb14de1-08fa-4e14-e230-cf47b038c338"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\anaconda3\\envs\\openmmlab\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:612: UserWarning: Checkpoint directory D:\\Development\\git_projects\\youtube-tutorials\\checkpoints exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type              | Params\n",
            "--------------------------------------------------\n",
            "0 | video_model | EfficientX3d      | 5.0 M \n",
            "1 | relu        | ReLU              | 0     \n",
            "2 | linear      | Linear            | 1.0 K \n",
            "3 | metric      | BinaryAccuracy    | 0     \n",
            "4 | criterion   | BCEWithLogitsLoss | 0     \n",
            "--------------------------------------------------\n",
            "5.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "5.0 M     Total params\n",
            "10.049    Total estimated model params size (MB)\n",
            "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Normal/Normal (12).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Normal/Normal (39).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>Normal/Normal (6).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Normal/Normal (4).mp4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>Shoplifting/Shoplifting (42).mp4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 path  label\n",
              "3              Normal/Normal (12).mp4      0\n",
              "32             Normal/Normal (39).mp4      0\n",
              "55              Normal/Normal (6).mp4      0\n",
              "33              Normal/Normal (4).mp4      0\n",
              "126  Shoplifting/Shoplifting (42).mp4      1"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "video=EncodedVideo.from_path('Normal/Normal (39).mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 20, 224, 224])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_data=video.get_clip(0,2)\n",
        "video_data=video_transform (video_data)\n",
        "video_data['video'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 20, 224, 224])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=model.cuda()\n",
        "inputs=video_data['video'].cuda()\n",
        "inputs=torch.unsqueeze(inputs,0)\n",
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.05938183]], dtype=float32)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds=model(inputs)\n",
        "preds=preds.detach().cpu().numpy()\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0]])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds=np.where(preds>0.5, 1,0)\n",
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds[0][0]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOk775iOkzpP5ZCIJzeG03V",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
